---
title: "RISE Simulation v. 1.0"
author: "John A. Graves"
output:
  html_document:
    code_folding: hide
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: yes
  html_notebook:
    code_folding: hide
    fig_caption: yes
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: yes
---

# Basic Scenario

This notebook tracks progress running the discrete event simulation for the NBER precision medicine meeting in September 2017.  The basis for the simulation is a "generic" scenario with the following parameters.

  - A population of 40-year old women are at risk for both secular death and an event (A) that happens at a **vRiskA** rate over a **vDurationA** year period.
  - All those who experience the event incur a cost of **A_c**.
  - Those who experience the event experience a **disutility\$A** utility decrement for **duration\$A** years, and are placed on a drug for life that costs **cost\$rx**/day.
  - There is a second event (B) that occurs downstream of Event A, with probability **vRiskB** over a **vDurationB** year period.
  - Event B has a **vFatalB** case fatality rate with a **cost\$B_Death** cost among the decedents and, among the survivors, incurs a **cost\$B_Survive** cost and a **disutility\$B_Survive** disutility for life.
  - There is a genetic test available that, if the person tests positive for the gene (prevalence is **vGene**), results in an alternative drug (for life) that costs **alt**/day but reduces the rate of the second event by a relative risk of **vRR_B**. 
  - The genetic test can be ordered as a single gene test (cost is **cost\$single_test**) or a panel test (cost **cost\$panel_test**).
  - Each individual is followed for up to 100 years or until death (either due to secular causese or case fatality). 
  
The simulation is set up such that it can be scaled to K scenarios with varying parameters for all the above.


```{r,eval=TRUE}
setwd("../")
rm(list=ls())
require(simmer)
require(dplyr)
require(tidyverse)
library(ellipse) 
run.id <- "vogi"
Strategies <- c("None-None","None-Single")
Strategies2 <- gsub("-",".",Strategies)
getwd()
for (s in Strategies) {
  temp <- unlist(strsplit(s,"-")) 
  load(file.path("./run-data/",paste0("ce-results-",run.id,"-",temp[1],"-",temp[2],".RData")))
  load(file.path("./run-data/",paste0("parameter-values-",run.id,"-",temp[1],"-",temp[2],".RData")))
  ce.results[[2]] <- cbind(ce.results[[2]],do.call("cbind",drawn.parameter.values)) 
  ce.results.overall <- ce.results[[1]]
  ce.results <- ce.results[[2]] %>% data.frame()
  #colnames(ce.results)[1:2] <- paste0(colnames(ce.results)[1:2],gsub("-",".",s))
  ce.results <- ce.results %>% mutate(strategy=s) %>% mutate(psa_id = row_number())
  assign(paste0("ce.results.",gsub("-",".",s)),ce.results)
  assign(paste0("ce.results.overall.",gsub("-",".",s)),ce.results.overall)
}

i = 1
for (i in seq(Strategies)) {
  if (i==1) ce.results <- get(paste0("ce.results.",Strategies2[i])) else
  {
    temp <- get(paste0("ce.results.",Strategies2[i]))
    ce.results <- rbind.data.frame(ce.results,temp)
  }
  i = i +1
}

varying1 <- suppressWarnings(ce.results %>% map_dbl(~var(.,na.rm=TRUE)) )
varying <- names(varying1[which(varying1>0)])
ce.results <- ce.results %>% select_at(c("psa_id","strategy",varying) )  %>% mutate(strategy=gsub("-",".",strategy))
id.vars <- c("psa_id","strategy",varying[-grep("psa_id|strategy|QALY|COST",varying)])
wide.fmla <- as.formula(paste0(paste0(c("psa_id",varying[-grep("psa_id|strategy|QALY|COST",varying)]),collapse="+"),"~variable+strategy"))
ce.results <- ce.results %>% reshape2::melt(id.vars=id.vars) %>% reshape2::dcast(wide.fmla)
```


```{r,eval=TRUE}
setwd("../")
source("./sub-files/metamodeling-functions.R")

#Determine the number of strategies
ndep <- length(Strategies)

#Create vector of variable names
Names <- ce.results %>% data.frame() %>% select(-psa_id) %>% colnames()
Parms <- ce.results[, -grep("psa_id|Iteration|QALY|COST", colnames(ce.results))]   
#Get parameter names
paramNames <- colnames(Parms)
indep <- ncol(Parms)
Outcomes <- ce.results[, grep("psa_id|strategy|QALY|COST", colnames(ce.results))] 

lambda <- 100000

for (i in seq(length(Strategies))) ce.results[,paste0("NHB_",Strategies2[i])] <- ce.results %>% select(-contains("NHB")) %>% select_if(grepl(Strategies2[i],names(.))) %>% mutate_at(vars(contains("COST")),funs(-1*./lambda)) %>% mutate(NHB=rowSums(.)) %>% pull(NHB)

for (i in seq(length(Strategies))) ce.results[,paste0("NMB_",Strategies2[i])] <- ce.results %>% select(-contains("NHB")) %>% select_if(grepl(Strategies2[i],names(.))) %>% mutate_at(vars(contains("COST")),funs(-1*.)) %>% mutate_at(vars(contains("QALY")),funs(lambda*.)) %>%  mutate(NMB=rowSums(.)) %>% pull(NMB)

ce.results <- ce.results %>% mutate(Iteration=row_number())

NHB <- ce.results %>% dplyr::select(dplyr::contains("NHB"))
NMB <- ce.results %>% dplyr::select(dplyr::contains("NMB"))
```


```{r}
nmb.gg <- reshape2::melt(NMB,  
               variable.name = "Strategy", 
               value.name = "NMB")
colnames(nmb.gg) <- c("Strategy","NMB")

## Plot NMB for different strategies
require(ggplot2)
require(scales)  # For dollar labels
require(grid)
number_ticks <- function(n) {function(limits) pretty(limits, n)} 
# Faceted plot by Strategy
ggplot(nmb.gg, aes(x = NMB/1000)) +
  geom_histogram(aes(y =..density..), col="black", fill = "gray") +
  geom_density(color = "red") +
  facet_wrap(~ Strategy, scales = "free_y") +
  xlab("Net Monetary Benefit (NMB) x10^3") +
  scale_x_continuous(breaks = number_ticks(5), labels = dollar) + 
  scale_y_continuous(breaks = number_ticks(5)) + 
  theme_bw()

```


```{r}
n.sim <- dim(NMB)[1]

Strategy1 <- "None.None"
Strategy2 <- "None.Single"

inmb <- NMB %>% select_at(vars(contains(Strategy1),contains(Strategy2))) %>% mutate_at(vars(contains(Strategy2)),funs(-1*.)) %>%  mutate(Net_NMB=rowSums(.)) %>% 
  mutate(Simulation = row_number()) %>% select(Simulation,Net_NMB)

#### Incremental NMB (INMB) ####
# Calculate INMB of B vs A
# Only B vs A but we could have plotted all combinations

## Format data frame suitably for plotting
inmb.gg <- reshape2::melt(inmb, id.vars = "Simulation", 
                variable.name = "Comparison", 
                value.name = "INMB")
colnames(inmb.gg) <- c("Simulation","Comparison","INMB")
txtsize<-16
## Plot INMB
ggplot(inmb.gg, aes(x = INMB/1000)) +
  geom_histogram(aes(y =..density..), col="black", fill = "gray") +
  geom_density(color = "red") +
  geom_vline(xintercept = 0, col = 4, size = 1.5, linetype = "dashed") +
  facet_wrap(~ Comparison, scales = "free_y") +
  xlab("Incremental Net Monetary Benefit (INMB) in thousand $") +
  scale_x_continuous(breaks = number_ticks(5), limits = c(-100, 100)) + 
  scale_y_continuous(breaks = number_ticks(5)) + 
  theme_bw(base_size = 14)
```


```{r}
#### Loss Matrix ####  
# Find optimal strategy (d*) based on the highest expected NMB
d.star <- which.max(colMeans(NMB))
d.star

# Or without iterating (much faster!)
loss <- as.matrix(NMB - NMB[, d.star])
head(loss)

#### EVPI ####
## Find maximum loss overall strategies at each state of the world 
## (i.e., PSA sample)
require(matrixStats)
max.loss.i <- rowMaxs(loss)
head(max.loss.i)
## Average across all states of the world
evpi <- mean(max.loss.i)
evpi

```






# Linear Metamodel

   The first example for linear metamodel will be on the outcomes (i.e., effectivenes, costs and NHB). We will run two versions of the metamodel based on how we incorporate the simulation parameters (**Important**: In the metamodel the simluation parameters will act as the independent variables): 
   
   1. With unstandardized values of the simulation paramaters $x_j$ and 
   2. With standardized values of the simulation paramaters $z_j$, defined as 
   $$z_j = \frac{x_j-\bar{x_j}}{\sigma_j}$$
   where $\bar{x_j}$ and $\sigma_j$ are the mean and standard deviation of $x_j$, respectively. We can standardized the simulation parameters within the linear regression specification in `R` using the command `scale`.
   
   
```{r,eval=FALSE}
require(tidyr)
require(broom)
# Unscaled #
Cost.Fit <- Strategies2 %>% purrr::map(~broom::tidy(lm(
  as.formula(paste0("dCOST_",.x,"~",paste0(colnames(Parms),collapse="+")))
  ,data=ce.results)))
Effectiveness.Fit <-Strategies2 %>% purrr::map(~tidy(lm(
  as.formula(paste0("dQALY_",.x,"~",paste0(colnames(Parms),collapse="+")))
  ,data=ce.results)))
NHB.Fit <- Strategies2 %>% purrr::map(~tidy(lm(
  as.formula(paste0("NHB_",.x,"~",paste0(colnames(Parms),collapse="+")))
  ,data=ce.results)))

# Scaled
Cost.Scaled.Fit <- Strategies2 %>% purrr::map(~tidy(lm(
  as.formula(paste0("dCOST_",.x,"~",paste0(paste0("scale(",colnames(Parms),")"),collapse="+")))
  ,data=ce.results)))
Effectiveness.Scaled.Fit <- Strategies2 %>% purrr::map(~tidy(lm(
  as.formula(paste0("dQALY_",.x,"~",paste0(paste0("scale(",colnames(Parms),")"),collapse="+")))
  ,data=ce.results)))
NHB.Scaled.Fit <- Strategies2 %>% purrr::map(~tidy(lm(
  as.formula(paste0("NHB_",.x,"~",paste0(paste0("scale(",colnames(Parms),")"),collapse="+")))
  ,data=ce.results)))


```

## One Way Sensitivity Analysis

```{r,eval=FALSE}
Sim.long<- reshape2::melt(ce.results, id.vars=c("Iteration",grep("Probability",Names,value=TRUE)),measure.vars=paste0("NHB_",Strategies2))
```

```{r,eval=FALSE,fig.cap="One Way Sensitivity Analysis"}
parm<-'global.vProbabilityOrder'
range<-c(0,1)
OneWaySA(Strategies,Parms,NHB,parm) 
```

## Threshold Analysis

```{r,eval=FALSE}
# Need to re-scale versions of each parameter
Parms %>% map_df(~mean(.x)) -> Parms.mean
Parms %>% map_df(~sd(.x)) -> Parms.sd

NHB.Scaled.Fit <- Strategies2 %>% purrr::map(~tidy(lm(
  as.formula(paste0("NHB_",.x,"~",paste0(paste0("scale(",colnames(Parms),")"),collapse="+")))
  ,data=ce.results)))
names(NHB.Scaled.Fit) = Strategies2
Strategy.Combinations <- t(combn(Strategies2,2))  %>% data.frame() %>% filter(as.character(X1)!=as.character(X2)) %>% 
  mutate_all(.funs=as.character)
delta <- map2(.x=as.character(Strategy.Combinations$X1),.y=as.character(Strategy.Combinations$X2),.f=~NHB.Scaled.Fit[[.x]]$estimate-NHB.Scaled.Fit[[.y]]$estimate) 
names(delta) = paste0(as.character(Strategy.Combinations$X1),"_vs_",as.character(Strategy.Combinations$X2))
delta <- delta %>% bind_rows()
zeta <- delta %>% purrr::map(~-.x[1]/.x[-1]) %>% bind_rows() 
zeta.rescaled <- t(Parms.mean)+t(Parms.sd)*zeta
rownames(zeta.rescaled) <- names(Parms)
```


```{r,eval=FALSE}
TornadoOpt(data.frame(Parms),data.frame(NHB))
```


```{r,eval=FALSE}
parm1<-'global.vProbabilityRead'
parm2<-'global.vProbabilityOrder'
range1<-c(0,1)
range2<-c(0,1)
TwoWaySA(Strategies,Parms,Outcomes=NHB,parm1,parm2,range1,range2)
```

```{r,eval=FALSE}
TornadoAll(Strategies2,Parms,NHB)
```


## Cost-Effectiveness Plane

```{r CE-Plane, fig.width=7, fig.height=6,warning=FALSE,eval=FALSE}
  PlaneCE(Strategies2,Outcomes)
```


```{r CE-Scatter, fig.width=7, fig.height=6,warning=FALSE,eval=FALSE}
  ScatterCE(Strategies,Outcomes)
```


```{r,eval=FALSE}
  lambda_range<-c(1000,150000,50)
```

```{r CEAC, fig.width=7, fig.height=6,warning=FALSE,eval=FALSE}
  CEAC(lambda_range,Strategies,Outcomes)
```



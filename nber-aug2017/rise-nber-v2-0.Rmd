---
title: "RISE Simulation v. 2.0"
author: "John A. Graves"
output:
  html_document:
    code_folding: hide
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: yes
  html_notebook:
    code_folding: hide
    fig_caption: yes
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: yes
---

# The Value of Geomic Information
```{r,message=FALSE,warning=FALSE}
lambda <- 125000
```

## Visualizing CE of Genetic Testing in a Cost-Effectiveness Plane

```{r blank-ce-plane,echo=FALSE,message=FALSE}
require(ggthemes)
require(tidyverse)
require(ellipse)
# One indication
# > vogi.none
#      dCOST    dQALY possible     fatal_b       living
# 1 4098.003 22.92677 23.01494 0.006469354 1.216035e-10
# > vogi.single
#      dCOST   dQALY possible     fatal_b       living
# 1 4688.283 22.9329 23.01705 0.006163024 1.216807e-10

# 20 Indications
# > vogi.none20
#      dCOST    dQALY possible   fatal_b       living
# 1 78616.71 20.62479 22.26546 0.1205602 8.977745e-11
# > vogi.single20
#      dCOST    dQALY possible   fatal_b       living
# 1 84948.41 20.73291 22.30471 0.1152217 9.092509e-11
# > 
  
dCost.none1 = 4098
dQALY.none1 = 22.92677
dCost.single1 = 4688.283
dQALY.single1 = 22.9329
dCost.none20 = 78616.71
dQALY.none20 = 20.62479 
dCost.single20 = 84948.41
dQALY.single20 = 20.73291

vogi1 <- (dQALY.single1-dQALY.none1)*lambda - (dCost.single1-dCost.none1)
vogi20 <- (dQALY.single20-dQALY.none20)*lambda - (dCost.single20-dCost.none20)
Means <- data.frame(delta_eff=c(dQALY.single1-dQALY.none1,dQALY.single20-dQALY.none20),
                    delta_cost=c(dCost.single1-dCost.none1,dCost.single20-dCost.none20),
                    scenario = c("Single Indication","20 Indications"))
txtsize <- 12

YLim <- c(-500,7000)
XLim <- c(-.05,.20)
Lambda.Pos <- YLim[2]/lambda

ggplot(Means,aes(x=delta_eff,y=delta_cost,colour=scenario))+geom_blank()+
  theme_tufte()+xlim(XLim)+ylim(YLim)+geom_point()+
  geom_hline(aes(yintercept=0))+geom_vline(aes(xintercept=0))+
  geom_abline(aes(intercept=0,slope=lambda),lty=3)+
  annotate("text",x=Lambda.Pos,y=YLim[2],label= paste0("lambda==100000"),hjust=-.25,parse=T)+
  ylab("Cost Change")+xlab("QALY Change")+
   scale_colour_discrete(l=50) 

```

Our estimate of the VOGI is based on the Net Monetary Benefit (NMB) as a summary measure of cost-effectiveness. NMB is a common metric summarizing (in dollar terms) the health benefits and costs of a given treatment alternative. For treatment option ($\alpha$) and model parameters $\mathbf{\pi} = \{ \pi_1 , ...., \pi_I \}$ π = define:

\[
(1) \quad \quad NMB(\alpha, \mathbf{\pi}) = \lambda \cdot Q(\alpha,\mathbf{\pi}) – C(\alpha, \mathbf{\pi}) 
\]

Where $Q(\alpha, \mathbf{\pi})$ is the QALY measure from the intervention, $C(\alpha, \mathbf{\pi})$ is the cost of the intervention, and $\lambda $ is the fixed value of the marginal societal willingness to pay for an incremental health improvement (i.e., \$50,000/QALY). For a given $ \lambda $ we define the optimal treatment as that which provides the maximum NMB on average:

\[ 
(2) \quad \quad \textrm{max}_\alpha \quad  E_{\mathbf{\pi}}[ \textrm{NMB}(\alpha, \mathbf{\pi})	] 
\]

When estimating equation (2) for the baseline no-genotyping scenario, all model parameters $\mathbf{\pi}=\{ \pi_1,..., \pi_I}$ are held fixed at their mean value.  In other words, we assume that clinical decisions on drug therapy are based on the average population-level risk of an adverse event and not on information on the patient’s individual risk, preferences or characteristics.  

Conversely, to study the value of information we directly model heterogeneity in $\mathbf{\pi}$ by drawing values from the joint probability distribution $p(\mathbf{\pi})$ at each iteration of the model.97 To estimate the EVGT we will utilize genetic data from the simulated patient cohort and model an alternative scenario in which clinicians obtain and act on information on genotype for that patient. The underlying genetic results used for these analyses demonstrate reproducibility of greater than 99%, including greater than 99% concordance and greater than 95% call rate. Under this “perfect information” scenario the physician utilizes information on each patient’s genetic makeup and chooses the optimal drug at the time of initial prescription, thus minimizing the risk of an adverse event. The average NMB under this scenario is value of perfect information on genotype and is given by:

\[
(3) \quad \quad \textrm{E}_{\mathbf{\pi}} [ \textrm{max}_{\alpha} \quad \textrm{NMB}(\alpha, \mathbf{\pi})]
\]

Where the expectation is taken over the joint probability distribution of $\mathbf{\pi}$ (which in this case is simply the distribution of patients with each drug phenotype).  

We obtain an estimate of the EVGT by taking the difference between equations (3) and (2): 

\[
 (4) \quad \quad \textrm{EVGT} =\textrm{E}_{\mathbf{\pi}} [ \textrm{max}_{\alpha} \quad \textrm{NMB}(\alpha, \mathbf{\pi})]- \textrm{max}_\alpha \quad  E_{\mathbf{\pi}}[ \textrm{NMB}(\alpha, \mathbf{\pi})	]
\]

The EVGT provides an estimate of the opportunity cost (in dollar terms) of failing to incorporate genetic information into clinical decisions at the point of prescribing.98  In other words, it tells us maximum society would be willing to pay (per patient) to implement genotype-tailored care.  An estimate of the total amount society (or a payer) would be willing to pay can be obtained by multiplying this value by the total number of patients and by the rate of indication incidence in the population.98 


```{r,cache=TRUE,warning=FALSE,message=FALSE}
run.dir <- "/Volumes/ACCRE/projects/rise-simulation/"
load(file.path(run.dir,"run-data/vogi-numerical/vogi-none-psa.RData"))
load(file.path(run.dir,"run-data/vogi-numerical/vogi-single-psa.RData"))
vogi.single.psa <- vogi.single.psa %>% mutate(strategy="single")

ce.results <-
  rbind(vogi.none.psa,vogi.single.psa) %>% mutate(strategy = gsub("-", ".", strategy)) 
varying1 <-
  suppressWarnings(ce.results %>% map_dbl( ~ var(., na.rm = TRUE)))
varying <- names(varying1[which(varying1 > 0)])
id.vars <-
  c("psa_id", "strategy", varying[-grep("psa_id|strategy|QALY|COST", varying)])
wide.fmla <-
  as.formula(paste0(paste0(c("psa_id", varying[-grep("psa_id|strategy|QALY|COST", varying)]), collapse =
                             "+"), "~variable+strategy"))
ce.results <-
  ce.results %>% reshape2::melt(id.vars = id.vars) %>% mutate(psa_id=as.character(psa_id)) %>% reshape2::dcast(wide.fmla)

```


```{r,message=FALSE,warning=FALSE}
source("../sub-files/metamodeling-functions.R")
Strategies <- c("none","single")
#Determine the number of strategies
ndep <- length(Strategies)

#Create vector of variable names
Names <- ce.results %>% data.frame() %>% dplyr::select(-psa_id) %>% colnames()
Parms <- ce.results[, -grep("psa_id|Iteration|QALY|COST", colnames(ce.results))]   
#Get parameter names
paramNames <- colnames(Parms)
indep <- ncol(Parms)
Outcomes <- ce.results[, grep("psa_id|strategy|QALY|COST", colnames(ce.results))] 

for (i in seq(length(Strategies)))
  ce.results[, paste0("NHB_", Strategies[i])] <-
  ce.results %>% dplyr::select(-dplyr::contains("NHB"),-dplyr::contains("NMB"),-contains("global.")) %>% dplyr::select_if(grepl(Strategies[i], names(.))) %>% mutate_at(vars(contains("COST")), funs(-1 * . / lambda)) %>% mutate(NHB = rowSums(.)) %>% pull(NHB)

for (i in seq(length(Strategies)))
  ce.results[, paste0("NMB_", Strategies[i])] <-
  ce.results %>% dplyr::select(-contains("NHB"),-contains("NMB"),-contains("global.")) %>% dplyr::select_if(grepl(Strategies[i], names(.))) %>% mutate_at(vars(contains("COST")), funs(-1 *.)) %>% mutate_at(vars(contains("QALY")), funs(lambda * .)) %>%  mutate(NMB =rowSums(.)) %>% pull(NMB)

ce.results <- ce.results %>% mutate(Iteration=row_number())

NHB <- ce.results %>% dplyr::select(dplyr::contains("NHB"))
NMB <- ce.results %>% dplyr::select(dplyr::contains("NMB"))
```


```{r,message=FALSE,warning=FALSE}
nmb.gg <- reshape2::melt(NMB,  
               variable.name = "Strategy", 
               value.name = "NMB")
colnames(nmb.gg) <- c("Strategy","NMB")

## Plot NMB for different strategies
require(ggplot2)
require(scales)  # For dollar labels
require(grid)
number_ticks <- function(n) {function(limits) pretty(limits, n)} 
# Faceted plot by Strategy
ggplot(nmb.gg, aes(x = NMB/1000)) +
  geom_histogram(aes(y =..density..), col="black", fill = "gray") +
  geom_density(color = "red") +
  facet_wrap(~ Strategy, scales = "free_y") +
  xlab("Net Monetary Benefit (NMB) x10^3") +
  scale_x_continuous(breaks = number_ticks(5), labels = dollar) + 
  scale_y_continuous(breaks = number_ticks(5)) + 
  theme_bw()
```

```{r,message=FALSE,warning=FALSE}
n.sim <- dim(NMB)[1]

Strategy1 <- "none"
Strategy2 <- "single"

inmb <- NMB %>% dplyr::select_at(vars(contains(Strategy1),contains(Strategy2))) %>% mutate_at(vars(contains(Strategy2)),funs(-1*.)) %>%  mutate(Net_NMB=rowSums(.)) %>% 
  mutate(Simulation = row_number()) %>% dplyr::select(Simulation,Net_NMB)

#### Incremental NMB (INMB) ####
# Calculate INMB of B vs A
# Only B vs A but we could have plotted all combinations

## Format data frame suitably for plotting
inmb.gg <- reshape2::melt(inmb, id.vars = "Simulation", 
                variable.name = "Comparison", 
                value.name = "INMB")
colnames(inmb.gg) <- c("Simulation","Comparison","INMB")
txtsize<-16
## Plot INMB
ggplot(inmb.gg, aes(x = INMB/1000)) +
  geom_histogram(aes(y =..density..), col="black", fill = "gray") +
  geom_density(color = "red") +
  geom_vline(xintercept = 0, col = 4, size = 1.5, linetype = "dashed") +
  facet_wrap(~ Comparison, scales = "free_y") +
  xlab("Incremental Net Monetary Benefit (INMB) in thousand $") +
  scale_x_continuous(breaks = number_ticks(5), limits = c(-100, 100)) + 
  scale_y_continuous(breaks = number_ticks(5)) + 
  theme_bw(base_size = 14)
```



```{r,message=FALSE,warning=FALSE}
#### Loss Matrix ####  
# Find optimal strategy (d*) based on the highest expected NMB
d.star <- which.max(colMeans(NMB))


# Or without iterating (much faster!)
loss <- as.matrix(NMB - NMB[, d.star])

#### EVPI ####
## Find maximum loss overall strategies at each state of the world 
## (i.e., PSA sample)
require(matrixStats)
max.loss.i <- rowMaxs(loss)

## Average across all states of the world
evpi <- mean(max.loss.i)


```


```{r,eval=FALSE}
require(tidyr)
require(broom)
# Unscaled #
Cost.Fit <- Strategies %>% purrr::map(~broom::tidy(lm(
  as.formula(paste0("dCOST_",.x,"~",paste0(colnames(Parms),collapse="+")))
  ,data=ce.results)))
Effectiveness.Fit <-Strategies %>% purrr::map(~tidy(lm(
  as.formula(paste0("dQALY_",.x,"~",paste0(colnames(Parms),collapse="+")))
  ,data=ce.results)))
NHB.Fit <- Strategies %>% purrr::map(~tidy(lm(
  as.formula(paste0("NHB_",.x,"~",paste0(colnames(Parms),collapse="+")))
  ,data=ce.results)))

# Scaled
Cost.Scaled.Fit <- Strategies %>% purrr::map(~tidy(lm(
  as.formula(paste0("dCOST_",.x,"~",paste0(paste0("scale(",colnames(Parms),")"),collapse="+")))
  ,data=ce.results)))
Effectiveness.Scaled.Fit <- Strategies %>% purrr::map(~tidy(lm(
  as.formula(paste0("dQALY_",.x,"~",paste0(paste0("scale(",colnames(Parms),")"),collapse="+")))
  ,data=ce.results)))
NHB.Scaled.Fit <- Strategies %>% purrr::map(~tidy(lm(
  as.formula(paste0("NHB_",.x,"~",paste0(paste0("scale(",colnames(Parms),")"),collapse="+")))
  ,data=ce.results)))


```


## One Way Sensitivity Analysis

```{r,eval=FALSE}
Sim.long<- reshape2::melt(ce.results, id.vars=c("Iteration",grep("Probability",Names,value=TRUE)),measure.vars=paste0("NHB_",Strategies))
```

```{r,eval=FALSE,fig.cap="One Way Sensitivity Analysis"}
parm<-'cost.alt_SC_A'
OneWaySA(Strategies,Parms,NHB,parm) 
```


```{r,eval=FALSE}
# Need to re-scale versions of each parameter
Parms %>% map_df(~mean(.x)) -> Parms.mean
Parms %>% map_df(~sd(.x)) -> Parms.sd

NHB.Scaled.Fit <- Strategies %>% purrr::map(~tidy(lm(
  as.formula(paste0("NHB_",.x,"~",paste0(paste0("scale(",colnames(Parms),")"),collapse="+")))
  ,data=ce.results)))
names(NHB.Scaled.Fit) = Strategies
Strategy.Combinations <- t(combn(Strategies,2))  %>% data.frame() %>% filter(as.character(X1)!=as.character(X2)) %>% 
  mutate_all(.funs=as.character)
delta <- map2(.x=as.character(Strategy.Combinations$X1),.y=as.character(Strategy.Combinations$X2),.f=~NHB.Scaled.Fit[[.x]]$estimate-NHB.Scaled.Fit[[.y]]$estimate) 
names(delta) = paste0(as.character(Strategy.Combinations$X1),"_vs_",as.character(Strategy.Combinations$X2))
delta <- delta %>% bind_rows()
zeta <- delta %>% purrr::map(~-.x[1]/.x[-1]) %>% bind_rows() 
zeta.rescaled <- t(Parms.mean)+t(Parms.sd)*zeta
rownames(zeta.rescaled) <- names(Parms)
```

## Tornado Diagram
```{r}
TornadoOpt(Parms=Parms,Outcomes=Outcomes[,-1])
```

```{r,message=FALSE,warning=FALSE}
TornadoAll(Strategies,Parms[,-which(names(Parms)=="global.single_test")],NHB)
```

## Cost-Effectiveness Scatterplot
```{r,message=FALSE,warning=FALSE}
 ScatterCE(Strategies,Outcomes)
```

## Two-Way Sensitivity Analysis
```{r,message=FALSE,warning=FALSE}
parm1<-'cost.alt_SC_A'
parm2<-'risk.vRiskB_SC_A'
range1<-range(Parms[,parm1])
range2<-range(Parms[,parm2])
TwoWaySA(Strategies,Parms,Outcomes=NHB,parm1,parm2,range1,range2)
```



